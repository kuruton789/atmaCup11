{"nbformat":4,"nbformat_minor":0,"metadata":{"accelerator":"GPU","colab":{"name":"simsiam_tutorial_2.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","metadata":{"id":"jnv4ExZ3kyPP"},"source":["# はじめに\n","このノートブックではSelf-Supervised Learning（以下SSL）の一種である**SimSiam**を用いて、与えられた美術作品に関しての表現学習を行います。SSLはラベルを利用せずデータそのものから擬似的に教師データを作ってデータの表現を学習する手法で、今回のように事前学習済み重みが禁止されている場合においても「より良い重み」を得るのに使える可能性があります。   \n","SSLの一種であるSimSiamは2つのネットワークに対して異なるデータ拡張を施して類似度を近づけるように学習させる手法で、負例を用意せずにシンプルに学習を行えるという特徴があります。  \n","<img src=\"https://tech.fusic.co.jp/uploads/exploring_siam_arcs.png\" width=60%>    \n","今回はSSLを簡単に行うことができるライブラリ**lightly**を用いてSimSiamによる表現学習を行います。公式のチュートリアルに従って、表現学習を行い、得られたembeddingからデータの分布を確認することが目的となります。  \n","(※Pretrained Modelは使用しないよう注意しましょう) \n","  \n","【参考資料】  \n","[Exploring Simple Siamese Representation Learning](https://arxiv.org/abs/2011.10566)  \n","[自己教師あり学習の新しいアプローチ / SimSiam: Exploring Simple Siamese Representation Learning](https://speakerdeck.com/sansandsoc/simsiam-exploring-simple-siamese-representation-learning)  \n","[【論文読み】Exploring Simple Siamese Representation Learning](https://tech.fusic.co.jp/posts/2020-12-25-ml-simsiam-representation-learning/)  \n","[https://github.com/lightly-ai/lightly](https://github.com/lightly-ai/lightly)  \n","[Train SimSiam on satellite images](https://docs.lightly.ai/tutorials/package/tutorial_simsiam_esa.html)"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"bi5OS6yM31ok","executionInfo":{"status":"ok","timestamp":1626568798721,"user_tz":-540,"elapsed":31533,"user":{"displayName":"クルトン","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgHntcg_d2qsxMa-rbEE6tvAWHvAPDuunfEdK0t=s64","userId":"17231302919530674825"}},"outputId":"4e7112f5-2d4b-4f56-d46d-8df598c1c407"},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Mounted at /content/drive\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"hXY9qYISD7AF","executionInfo":{"status":"ok","timestamp":1626568812955,"user_tz":-540,"elapsed":14242,"user":{"displayName":"クルトン","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgHntcg_d2qsxMa-rbEE6tvAWHvAPDuunfEdK0t=s64","userId":"17231302919530674825"}},"outputId":"eb91d583-5c96-48cf-f3bf-424ddfaf6c22"},"source":["# For Colab\n","!pip install lightly"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Collecting lightly\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/86/8f/0345b730f0f06a3bfb6b1e04ea1b132ef302cc00b6456bbfdb9c5a5015d6/lightly-1.1.15-py3-none-any.whl (240kB)\n","\r\u001b[K     |█▍                              | 10kB 19.5MB/s eta 0:00:01\r\u001b[K     |██▊                             | 20kB 25.4MB/s eta 0:00:01\r\u001b[K     |████                            | 30kB 30.2MB/s eta 0:00:01\r\u001b[K     |█████▌                          | 40kB 24.1MB/s eta 0:00:01\r\u001b[K     |██████▉                         | 51kB 17.0MB/s eta 0:00:01\r\u001b[K     |████████▏                       | 61kB 14.2MB/s eta 0:00:01\r\u001b[K     |█████████▌                      | 71kB 15.7MB/s eta 0:00:01\r\u001b[K     |███████████                     | 81kB 15.1MB/s eta 0:00:01\r\u001b[K     |████████████▎                   | 92kB 16.3MB/s eta 0:00:01\r\u001b[K     |█████████████▋                  | 102kB 15.2MB/s eta 0:00:01\r\u001b[K     |███████████████                 | 112kB 15.2MB/s eta 0:00:01\r\u001b[K     |████████████████▍               | 122kB 15.2MB/s eta 0:00:01\r\u001b[K     |█████████████████▊              | 133kB 15.2MB/s eta 0:00:01\r\u001b[K     |███████████████████             | 143kB 15.2MB/s eta 0:00:01\r\u001b[K     |████████████████████▌           | 153kB 15.2MB/s eta 0:00:01\r\u001b[K     |█████████████████████▉          | 163kB 15.2MB/s eta 0:00:01\r\u001b[K     |███████████████████████▏        | 174kB 15.2MB/s eta 0:00:01\r\u001b[K     |████████████████████████▋       | 184kB 15.2MB/s eta 0:00:01\r\u001b[K     |██████████████████████████      | 194kB 15.2MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▎    | 204kB 15.2MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▋   | 215kB 15.2MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████  | 225kB 15.2MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▍| 235kB 15.2MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 245kB 15.2MB/s \n","\u001b[?25hRequirement already satisfied: torchvision in /usr/local/lib/python3.7/dist-packages (from lightly) (0.10.0+cu102)\n","Requirement already satisfied: setuptools>=21.0.0 in /usr/local/lib/python3.7/dist-packages (from lightly) (57.2.0)\n","Requirement already satisfied: six>=1.10 in /usr/local/lib/python3.7/dist-packages (from lightly) (1.15.0)\n","Collecting lightly-utils==0.0.1\n","  Downloading https://files.pythonhosted.org/packages/17/a1/e36f214e3d22d8417b718f51cdc49853bd63a50584f13981926ef8c5a368/lightly_utils-0.0.1-py3-none-any.whl\n","Requirement already satisfied: certifi>=14.05.14 in /usr/local/lib/python3.7/dist-packages (from lightly) (2021.5.30)\n","Requirement already satisfied: requests>=2.23.0 in /usr/local/lib/python3.7/dist-packages (from lightly) (2.23.0)\n","Requirement already satisfied: python-dateutil>=2.5.3 in /usr/local/lib/python3.7/dist-packages (from lightly) (2.8.1)\n","Collecting tqdm>=4.44\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7a/ec/f8ff3ccfc4e59ce619a66a0bf29dc3b49c2e8c07de29d572e191c006eaa2/tqdm-4.61.2-py2.py3-none-any.whl (76kB)\n","\u001b[K     |████████████████████████████████| 81kB 11.0MB/s \n","\u001b[?25hCollecting pytorch-lightning>=1.0.4\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/48/5e/19c817ad2670c1d822642ed7bfc4d9d4c30c2f8eaefebcd575a3188d7319/pytorch_lightning-1.3.8-py3-none-any.whl (813kB)\n","\u001b[K     |████████████████████████████████| 819kB 29.3MB/s \n","\u001b[?25hCollecting hydra-core>=1.0.0\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/c3/cd/85aa2e3a8babc36feac99df785e54abf99afbc4acc20488630f3ef46980a/hydra_core-1.1.0-py3-none-any.whl (144kB)\n","\u001b[K     |████████████████████████████████| 153kB 45.7MB/s \n","\u001b[?25hRequirement already satisfied: urllib3>=1.15.1 in /usr/local/lib/python3.7/dist-packages (from lightly) (1.24.3)\n","Requirement already satisfied: numpy>=1.18.1 in /usr/local/lib/python3.7/dist-packages (from lightly) (1.19.5)\n","Requirement already satisfied: pillow>=5.3.0 in /usr/local/lib/python3.7/dist-packages (from torchvision->lightly) (7.1.2)\n","Requirement already satisfied: torch==1.9.0 in /usr/local/lib/python3.7/dist-packages (from torchvision->lightly) (1.9.0+cu102)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests>=2.23.0->lightly) (2.10)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests>=2.23.0->lightly) (3.0.4)\n","Collecting fsspec[http]!=2021.06.0,>=2021.05.0\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/40/e1/7111d8afc76ee3171f4f99592cd29bac9d233ae1aa34623011506f955434/fsspec-2021.7.0-py3-none-any.whl (118kB)\n","\u001b[K     |████████████████████████████████| 122kB 50.9MB/s \n","\u001b[?25hCollecting pyDeprecate==0.3.0\n","  Downloading https://files.pythonhosted.org/packages/14/52/aa227a0884df71ed1957649085adf2b8bc2a1816d037c2f18b3078854516/pyDeprecate-0.3.0-py3-none-any.whl\n","Requirement already satisfied: packaging>=17.0 in /usr/local/lib/python3.7/dist-packages (from pytorch-lightning>=1.0.4->lightly) (21.0)\n","Collecting tensorboard!=2.5.0,>=2.2.0\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/64/21/eebd23060763fedeefb78bc2b286e00fa1d8abda6f70efa2ee08c28af0d4/tensorboard-2.4.1-py3-none-any.whl (10.6MB)\n","\u001b[K     |████████████████████████████████| 10.6MB 48.9MB/s \n","\u001b[?25hCollecting future>=0.17.1\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/45/0b/38b06fd9b92dc2b68d58b75f900e97884c45bedd2ff83203d933cf5851c9/future-0.18.2.tar.gz (829kB)\n","\u001b[K     |████████████████████████████████| 829kB 49.5MB/s \n","\u001b[?25hCollecting PyYAML<=5.4.1,>=5.1\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7a/a5/393c087efdc78091afa2af9f1378762f9821c9c1d7a22c5753fb5ac5f97a/PyYAML-5.4.1-cp37-cp37m-manylinux1_x86_64.whl (636kB)\n","\u001b[K     |████████████████████████████████| 645kB 44.4MB/s \n","\u001b[?25hCollecting torchmetrics>=0.2.0\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/4d/8b/de8df9044ca2ac5dfc6b13b9ad3b3ebe6b3a45807311102b569d680e811f/torchmetrics-0.4.1-py3-none-any.whl (234kB)\n","\u001b[K     |████████████████████████████████| 235kB 51.3MB/s \n","\u001b[?25hCollecting antlr4-python3-runtime==4.8\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/56/02/789a0bddf9c9b31b14c3e79ec22b9656185a803dc31c15f006f9855ece0d/antlr4-python3-runtime-4.8.tar.gz (112kB)\n","\u001b[K     |████████████████████████████████| 112kB 54.7MB/s \n","\u001b[?25hRequirement already satisfied: importlib-resources; python_version < \"3.9\" in /usr/local/lib/python3.7/dist-packages (from hydra-core>=1.0.0->lightly) (5.2.0)\n","Collecting omegaconf==2.1.*\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f9/96/1966b48bfe6ca64bfadfa7bcc9a8d73c5d83b4be769321fcc5d617abeb0c/omegaconf-2.1.0-py3-none-any.whl (74kB)\n","\u001b[K     |████████████████████████████████| 81kB 9.3MB/s \n","\u001b[?25hRequirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch==1.9.0->torchvision->lightly) (3.7.4.3)\n","Collecting aiohttp; extra == \"http\"\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/88/c0/5890b4c8b04a79b7360e8fe4490feb0bb3ab179743f199f0e6220cebd568/aiohttp-3.7.4.post0-cp37-cp37m-manylinux2014_x86_64.whl (1.3MB)\n","\u001b[K     |████████████████████████████████| 1.3MB 37.7MB/s \n","\u001b[?25hRequirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=17.0->pytorch-lightning>=1.0.4->lightly) (2.4.7)\n","Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard!=2.5.0,>=2.2.0->pytorch-lightning>=1.0.4->lightly) (0.4.4)\n","Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.7/dist-packages (from tensorboard!=2.5.0,>=2.2.0->pytorch-lightning>=1.0.4->lightly) (0.12.0)\n","Requirement already satisfied: protobuf>=3.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard!=2.5.0,>=2.2.0->pytorch-lightning>=1.0.4->lightly) (3.17.3)\n","Requirement already satisfied: wheel>=0.26; python_version >= \"3\" in /usr/local/lib/python3.7/dist-packages (from tensorboard!=2.5.0,>=2.2.0->pytorch-lightning>=1.0.4->lightly) (0.36.2)\n","Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard!=2.5.0,>=2.2.0->pytorch-lightning>=1.0.4->lightly) (1.8.0)\n","Requirement already satisfied: grpcio>=1.24.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard!=2.5.0,>=2.2.0->pytorch-lightning>=1.0.4->lightly) (1.34.1)\n","Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard!=2.5.0,>=2.2.0->pytorch-lightning>=1.0.4->lightly) (3.3.4)\n","Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard!=2.5.0,>=2.2.0->pytorch-lightning>=1.0.4->lightly) (1.0.1)\n","Requirement already satisfied: google-auth<2,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard!=2.5.0,>=2.2.0->pytorch-lightning>=1.0.4->lightly) (1.32.1)\n","Requirement already satisfied: zipp>=3.1.0; python_version < \"3.10\" in /usr/local/lib/python3.7/dist-packages (from importlib-resources; python_version < \"3.9\"->hydra-core>=1.0.0->lightly) (3.5.0)\n","Collecting multidict<7.0,>=4.5\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7c/a6/4123b8165acbe773d1a8dc8e3f0d1edea16d29f7de018eda769abb56bd30/multidict-5.1.0-cp37-cp37m-manylinux2014_x86_64.whl (142kB)\n","\u001b[K     |████████████████████████████████| 143kB 57.7MB/s \n","\u001b[?25hCollecting yarl<2.0,>=1.0\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f1/62/046834c5fc998c88ab2ef722f5d42122230a632212c8afa76418324f53ff/yarl-1.6.3-cp37-cp37m-manylinux2014_x86_64.whl (294kB)\n","\u001b[K     |████████████████████████████████| 296kB 51.3MB/s \n","\u001b[?25hRequirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp; extra == \"http\"->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning>=1.0.4->lightly) (21.2.0)\n","Collecting async-timeout<4.0,>=3.0\n","  Downloading https://files.pythonhosted.org/packages/e1/1e/5a4441be21b0726c4464f3f23c8b19628372f606755a9d2e46c187e65ec4/async_timeout-3.0.1-py3-none-any.whl\n","Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard!=2.5.0,>=2.2.0->pytorch-lightning>=1.0.4->lightly) (1.3.0)\n","Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard!=2.5.0,>=2.2.0->pytorch-lightning>=1.0.4->lightly) (4.6.1)\n","Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard!=2.5.0,>=2.2.0->pytorch-lightning>=1.0.4->lightly) (0.2.8)\n","Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard!=2.5.0,>=2.2.0->pytorch-lightning>=1.0.4->lightly) (4.2.2)\n","Requirement already satisfied: rsa<5,>=3.1.4; python_version >= \"3.6\" in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard!=2.5.0,>=2.2.0->pytorch-lightning>=1.0.4->lightly) (4.7.2)\n","Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard!=2.5.0,>=2.2.0->pytorch-lightning>=1.0.4->lightly) (3.1.1)\n","Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.3->tensorboard!=2.5.0,>=2.2.0->pytorch-lightning>=1.0.4->lightly) (0.4.8)\n","Building wheels for collected packages: future, antlr4-python3-runtime\n","  Building wheel for future (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for future: filename=future-0.18.2-cp37-none-any.whl size=491070 sha256=bf2b9b729f6987f9cd5f1de85490fbf284cf8e90f46d22a15b92f7f70e553e4c\n","  Stored in directory: /root/.cache/pip/wheels/8b/99/a0/81daf51dcd359a9377b110a8a886b3895921802d2fc1b2397e\n","  Building wheel for antlr4-python3-runtime (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for antlr4-python3-runtime: filename=antlr4_python3_runtime-4.8-cp37-none-any.whl size=141231 sha256=f992c2e336738cb64fccb42414e9f3d665484d2dc2eb99e90b5ff06335cd7ea1\n","  Stored in directory: /root/.cache/pip/wheels/e3/e2/fa/b78480b448b8579ddf393bebd3f47ee23aa84c89b6a78285c8\n","Successfully built future antlr4-python3-runtime\n","\u001b[31mERROR: tensorflow 2.5.0 has requirement tensorboard~=2.5, but you'll have tensorboard 2.4.1 which is incompatible.\u001b[0m\n","Installing collected packages: lightly-utils, tqdm, multidict, yarl, async-timeout, aiohttp, fsspec, pyDeprecate, tensorboard, future, PyYAML, torchmetrics, pytorch-lightning, antlr4-python3-runtime, omegaconf, hydra-core, lightly\n","  Found existing installation: tqdm 4.41.1\n","    Uninstalling tqdm-4.41.1:\n","      Successfully uninstalled tqdm-4.41.1\n","  Found existing installation: tensorboard 2.5.0\n","    Uninstalling tensorboard-2.5.0:\n","      Successfully uninstalled tensorboard-2.5.0\n","  Found existing installation: future 0.16.0\n","    Uninstalling future-0.16.0:\n","      Successfully uninstalled future-0.16.0\n","  Found existing installation: PyYAML 3.13\n","    Uninstalling PyYAML-3.13:\n","      Successfully uninstalled PyYAML-3.13\n","Successfully installed PyYAML-5.4.1 aiohttp-3.7.4.post0 antlr4-python3-runtime-4.8 async-timeout-3.0.1 fsspec-2021.7.0 future-0.18.2 hydra-core-1.1.0 lightly-1.1.15 lightly-utils-0.0.1 multidict-5.1.0 omegaconf-2.1.0 pyDeprecate-0.3.0 pytorch-lightning-1.3.8 tensorboard-2.4.1 torchmetrics-0.4.1 tqdm-4.61.2 yarl-1.6.3\n"],"name":"stdout"},{"output_type":"display_data","data":{"application/vnd.colab-display-data+json":{"pip_warning":{"packages":["pydevd_plugins"]}}},"metadata":{"tags":[]}}]},{"cell_type":"code","metadata":{"id":"KbW80FmuD88C"},"source":["# For Colab\n","# !unzip dataset_atmaCup11.zip\n","# !mkdir imgs\n","# !unzip photos.zip -d ./imgs/"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"vZsrVp2tpyLX"},"source":["# SimSiamによる表現学習"]},{"cell_type":"markdown","metadata":{"id":"FOdMJmECpDSK"},"source":["### ライブラリの読み込み"]},{"cell_type":"code","metadata":{"id":"11cSEVpQLYQo"},"source":["import math\n","import torch\n","import torch.nn as nn\n","import torchvision\n","import numpy as np\n","import lightly"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"8ov_gUFtpFvg"},"source":["### Config"]},{"cell_type":"markdown","metadata":{"id":"zrUObl-NtVCC"},"source":["- `batch_size`はより大きい方がよいかもしれません（論文参照）\n","- `num_ftrs`は画像認識モデルのembedding数を表しており、今回は`resnet18`を使用しているため512となります"]},{"cell_type":"code","metadata":{"id":"FAR0agY4LjPw"},"source":["use_amp = True\n","\n","num_workers = 2\n","batch_size = 512\n","seed = 1\n","epochs = 500\n","input_size = 224\n","\n","# dimension of the embeddings\n","num_ftrs = 512\n","# dimension of the output of the prediction and projection heads\n","out_dim = proj_hidden_dim = 512\n","# the prediction head uses a bottleneck architecture\n","pred_hidden_dim = 128\n","# use 2 layers in the projection head\n","num_mlp_layers = 2"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"4ReM7luSpJeb"},"source":["### Seed / 画像データのPath"]},{"cell_type":"code","metadata":{"id":"ItkMXZzDL2Ny"},"source":["\n","\n","# seed torch and numpy\n","torch.manual_seed(0)\n","np.random.seed(0)\n","\n","# set the path to the dataset\n","\n","path_to_data = '/content/drive/MyDrive/atmaCup/#11/dataset_atmaCup11/inputs/photos'"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"e7ae4uAXpVeo"},"source":["### DataLoader"]},{"cell_type":"markdown","metadata":{"id":"izlrXBVuuHG_"},"source":["- `collate_fn`でDataAugmentationと適用される確率を指定しています。\n","  - hf_prob: Horizontal flip\n","  - vf_prob: Vertical flip\n","  - rr_prob: Random (+90 degree) rotation\n","  - min_scale: Random Cropの最小スケール\n","  - cj_prob: Color jitter\n","  - cj_bright: Brightness jitter\n","  - cj_contrast: Contrast jitter\n","  - cj_hue: Hue jitter\n","  - cj_sat: Saturation jitter\n","- 参考： https://docs.lightly.ai/lightly.data.html#lightly.data.collate.ImageCollateFunction"]},{"cell_type":"code","metadata":{"id":"BYc-ZnxdL-_D"},"source":["# define the augmentations for self-supervised learning\n","collate_fn = lightly.data.ImageCollateFunction(\n","    input_size=input_size,\n","    # require invariance to flips and rotations\n","    hf_prob=0.5,\n","    vf_prob=0.5,\n","    rr_prob=0.5,\n","    # satellite images are all taken from the same height\n","    # so we use only slight random cropping\n","    min_scale=0.5,\n","    # use a weak color jitter for invariance w.r.t small color changes\n","    # cj_prob=0.2,\n","    # cj_bright=0.1,\n","    # cj_contrast=0.1,\n","    # cj_hue=0.1,\n","    # cj_sat=0.1,\n",")\n","\n","# create a lightly dataset for training, since the augmentations are handled\n","# by the collate function, there is no need to apply additional ones here\n","dataset_train_simsiam = lightly.data.LightlyDataset(\n","    input_dir=path_to_data\n",")\n","\n","# create a dataloader for training\n","dataloader_train_simsiam = torch.utils.data.DataLoader(\n","    dataset_train_simsiam,\n","    batch_size=batch_size,\n","    shuffle=True,\n","    collate_fn=collate_fn,\n","    drop_last=True,\n","    num_workers=num_workers\n",")\n","\n","# create a torchvision transformation for embedding the dataset after training\n","# here, we resize the images to match the input size during training and apply\n","# a normalization of the color channel based on statistics from imagenet\n","test_transforms = torchvision.transforms.Compose([\n","    torchvision.transforms.Resize((input_size, input_size)),\n","    torchvision.transforms.ToTensor(),\n","    torchvision.transforms.Normalize(\n","        mean=lightly.data.collate.imagenet_normalize['mean'],\n","        std=lightly.data.collate.imagenet_normalize['std'],\n","    )\n","])\n","\n","\n","\n","# create a lightly dataset for embedding\n","dataset_test = lightly.data.LightlyDataset(\n","    input_dir=path_to_data,\n","    transform=test_transforms\n",")\n","\n","\n","\n","# create a dataloader for embedding\n","dataloader_test = torch.utils.data.DataLoader(\n","    dataset_test,\n","    batch_size=batch_size,\n","    shuffle=False,\n","    drop_last=False,\n","    num_workers=num_workers\n",")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"yiD3kOO1paXy"},"source":["### Model"]},{"cell_type":"markdown","metadata":{"id":"GZCF6kbEvyNa"},"source":["- ResNet18の最終層を取り除くことでBackboneレイヤーとしています\n","- ※Pretrained Modelは使用しないよう注意しましょう"]},{"cell_type":"code","metadata":{"id":"Sy_KEl78MNkX"},"source":["# we use a pretrained resnet for this tutorial to speed\n","# up training time but you can also train one from scratch\n","# Do not use pretrained Model\n","resnet = torchvision.models.resnet18(pretrained=False)\n","backbone = nn.Sequential(*list(resnet.children())[:-1])\n","\n","# create the SimSiam model using the backbone from above\n","model = lightly.models.SimSiam(\n","    backbone,\n","    num_ftrs=num_ftrs,\n","    #proj_hidden_dim=proj_hidden_dim, # defaultを使用\n","    #pred_hidden_dim=pred_hidden_dim, # defaultを使用\n","    #out_dim=out_dim, # defaultを使用\n","    num_mlp_layers=2\n",")\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"1IK9HfbbpcPk"},"source":["### Loss / Optimizer"]},{"cell_type":"code","metadata":{"id":"Taf1bDjoMSTi"},"source":["# SimSiam uses a symmetric negative cosine similarity loss\n","criterion = lightly.loss.SymNegCosineSimilarityLoss()\n","\n","# scale the learning rate\n","lr = 0.05 * batch_size / 256\n","# use SGD with momentum and weight decay\n","optimizer = torch.optim.SGD(\n","    model.parameters(),\n","    lr=lr,\n","    momentum=0.9,\n","    weight_decay=5e-4\n",")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"aBgbatRe3GUQ"},"source":["scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, epochs)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"2MMdONCbpoY-"},"source":["### SimSiamによるSelf-Supervised Learning"]},{"cell_type":"code","metadata":{"id":"T86mDaAQ44gn"},"source":["import os\n","\n","dataset_root = '/content/drive/MyDrive/atmaCup/#11/dataset_atmaCup11'\n","assert dataset_root is not None\n","output_dir = os.path.join(dataset_root, \"simsam_tutorial\")\n","os.makedirs(output_dir, exist_ok=True)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"tPNDGsQdMViD","outputId":"fdeff53a-0d9c-46a5-996f-e92c3834e1b2"},"source":["device = 'cuda' if torch.cuda.is_available() else 'cpu'\n","model.to(device)\n","\n","avg_losses = []\n","avg_loss = 0.\n","avg_output_std = 0.\n","for e in range(1, epochs + 1):\n","\n","    for (x0, x1), _, _ in dataloader_train_simsiam:\n","        # move images to the gpu\n","        x0 = x0.to(device)\n","        x1 = x1.to(device)\n","        \n","        with torch.cuda.amp.autocast(enabled=use_amp):\n","            # run the model on both transforms of the images\n","            # the output of the simsiam model is a y containing the predictions\n","            # and projections for each input x\n","            y0, y1 = model(x0, x1)\n","\n","            # backpropagation\n","            loss = criterion(y0, y1)\n","        loss.backward()\n","\n","        optimizer.step()\n","        optimizer.zero_grad()\n","\n","        # calculate the per-dimension standard deviation of the outputs\n","        # we can use this later to check whether the embeddings are collapsing\n","        output, _ = y0\n","        output = output.detach()\n","        output = torch.nn.functional.normalize(output, dim=1)\n","\n","        output_std = torch.std(output, 0)\n","        output_std = output_std.mean()\n","\n","        # use moving averages to track the loss and standard deviation\n","        w = 0.9\n","        avg_loss = w * avg_loss + (1 - w) * loss.item()\n","        avg_output_std = w * avg_output_std + (1 - w) * output_std.item()\n","    \n","    scheduler.step()\n","\n","    # the level of collapse is large if the standard deviation of the l2\n","    # normalized output is much smaller than 1 / sqrt(dim)\n","    collapse_level = max(0., 1 - math.sqrt(out_dim) * avg_output_std)\n","    # print intermediate results\n","    print(f'[Epoch {e:3d}] '\n","        f'Loss = {avg_loss:.2f} | '\n","        f'Collapse Level: {collapse_level:.2f} / 1.00')\n","    \n","    avg_losses.append(avg_loss)\n","    \n","    if e % 50 == 0:\n","        model_path = os.path.join(output_dir, str(batch_size) + '_' + str(e) + '_' + str(input_size) + '_' + str(avg_loss) + '.pth')\n","        torch.save(model.state_dict(), model_path)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:718: UserWarning: Named tensors and all their associated APIs are an experimental feature and subject to change. Please do not use them for anything important until they are released as stable. (Triggered internally at  /pytorch/c10/core/TensorImpl.h:1156.)\n","  return torch.max_pool2d(input, kernel_size, stride, padding, dilation, ceil_mode)\n"],"name":"stderr"},{"output_type":"stream","text":["[Epoch   1] Loss = -0.07 | Collapse Level: 0.59 / 1.00\n","[Epoch   2] Loss = -0.43 | Collapse Level: 0.60 / 1.00\n","[Epoch   3] Loss = -0.52 | Collapse Level: 0.60 / 1.00\n","[Epoch   4] Loss = -0.51 | Collapse Level: 0.59 / 1.00\n","[Epoch   5] Loss = -0.44 | Collapse Level: 0.57 / 1.00\n","[Epoch   6] Loss = -0.44 | Collapse Level: 0.58 / 1.00\n","[Epoch   7] Loss = -0.44 | Collapse Level: 0.58 / 1.00\n","[Epoch   8] Loss = -0.43 | Collapse Level: 0.57 / 1.00\n","[Epoch   9] Loss = -0.44 | Collapse Level: 0.57 / 1.00\n","[Epoch  10] Loss = -0.47 | Collapse Level: 0.58 / 1.00\n","[Epoch  11] Loss = -0.47 | Collapse Level: 0.57 / 1.00\n","[Epoch  12] Loss = -0.47 | Collapse Level: 0.57 / 1.00\n","[Epoch  13] Loss = -0.47 | Collapse Level: 0.57 / 1.00\n","[Epoch  14] Loss = -0.47 | Collapse Level: 0.57 / 1.00\n","[Epoch  15] Loss = -0.46 | Collapse Level: 0.56 / 1.00\n","[Epoch  16] Loss = -0.52 | Collapse Level: 0.58 / 1.00\n","[Epoch  17] Loss = -0.48 | Collapse Level: 0.56 / 1.00\n","[Epoch  18] Loss = -0.51 | Collapse Level: 0.58 / 1.00\n","[Epoch  19] Loss = -0.48 | Collapse Level: 0.56 / 1.00\n","[Epoch  20] Loss = -0.52 | Collapse Level: 0.57 / 1.00\n","[Epoch  21] Loss = -0.55 | Collapse Level: 0.58 / 1.00\n","[Epoch  22] Loss = -0.51 | Collapse Level: 0.57 / 1.00\n","[Epoch  23] Loss = -0.52 | Collapse Level: 0.57 / 1.00\n","[Epoch  24] Loss = -0.51 | Collapse Level: 0.56 / 1.00\n","[Epoch  25] Loss = -0.55 | Collapse Level: 0.57 / 1.00\n","[Epoch  26] Loss = -0.53 | Collapse Level: 0.57 / 1.00\n","[Epoch  27] Loss = -0.50 | Collapse Level: 0.55 / 1.00\n","[Epoch  28] Loss = -0.49 | Collapse Level: 0.55 / 1.00\n","[Epoch  29] Loss = -0.50 | Collapse Level: 0.54 / 1.00\n","[Epoch  30] Loss = -0.49 | Collapse Level: 0.53 / 1.00\n","[Epoch  31] Loss = -0.49 | Collapse Level: 0.52 / 1.00\n","[Epoch  32] Loss = -0.53 | Collapse Level: 0.53 / 1.00\n","[Epoch  33] Loss = -0.53 | Collapse Level: 0.52 / 1.00\n","[Epoch  34] Loss = -0.55 | Collapse Level: 0.52 / 1.00\n","[Epoch  35] Loss = -0.55 | Collapse Level: 0.52 / 1.00\n","[Epoch  36] Loss = -0.57 | Collapse Level: 0.52 / 1.00\n","[Epoch  37] Loss = -0.58 | Collapse Level: 0.52 / 1.00\n","[Epoch  38] Loss = -0.58 | Collapse Level: 0.52 / 1.00\n","[Epoch  39] Loss = -0.59 | Collapse Level: 0.52 / 1.00\n","[Epoch  40] Loss = -0.60 | Collapse Level: 0.52 / 1.00\n","[Epoch  41] Loss = -0.61 | Collapse Level: 0.52 / 1.00\n","[Epoch  42] Loss = -0.60 | Collapse Level: 0.52 / 1.00\n","[Epoch  43] Loss = -0.60 | Collapse Level: 0.52 / 1.00\n","[Epoch  44] Loss = -0.61 | Collapse Level: 0.52 / 1.00\n","[Epoch  45] Loss = -0.61 | Collapse Level: 0.52 / 1.00\n","[Epoch  46] Loss = -0.62 | Collapse Level: 0.52 / 1.00\n","[Epoch  47] Loss = -0.62 | Collapse Level: 0.52 / 1.00\n","[Epoch  48] Loss = -0.62 | Collapse Level: 0.52 / 1.00\n","[Epoch  49] Loss = -0.61 | Collapse Level: 0.52 / 1.00\n","[Epoch  50] Loss = -0.62 | Collapse Level: 0.51 / 1.00\n","[Epoch  51] Loss = -0.63 | Collapse Level: 0.52 / 1.00\n","[Epoch  52] Loss = -0.63 | Collapse Level: 0.52 / 1.00\n","[Epoch  53] Loss = -0.63 | Collapse Level: 0.52 / 1.00\n","[Epoch  54] Loss = -0.62 | Collapse Level: 0.51 / 1.00\n","[Epoch  55] Loss = -0.63 | Collapse Level: 0.51 / 1.00\n","[Epoch  56] Loss = -0.63 | Collapse Level: 0.51 / 1.00\n","[Epoch  57] Loss = -0.64 | Collapse Level: 0.52 / 1.00\n","[Epoch  58] Loss = -0.64 | Collapse Level: 0.51 / 1.00\n","[Epoch  59] Loss = -0.64 | Collapse Level: 0.51 / 1.00\n","[Epoch  60] Loss = -0.66 | Collapse Level: 0.51 / 1.00\n","[Epoch  61] Loss = -0.66 | Collapse Level: 0.51 / 1.00\n","[Epoch  62] Loss = -0.66 | Collapse Level: 0.51 / 1.00\n","[Epoch  63] Loss = -0.67 | Collapse Level: 0.51 / 1.00\n","[Epoch  64] Loss = -0.68 | Collapse Level: 0.51 / 1.00\n","[Epoch  65] Loss = -0.67 | Collapse Level: 0.51 / 1.00\n","[Epoch  66] Loss = -0.68 | Collapse Level: 0.51 / 1.00\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"VmDfkWMAEwup"},"source":["fig = plt.figure()\n","\n","plt.plot(list(range(1, epochs)), avg_losses)\n","\n","fig.savefig(os.path.join(output_dir, \"avg_losses_(batch_size: \" + str(batch_size) + \", epochs: \" + str(epochs) + \", input_size: \" + str(input_size) + \").png\"))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"66Re4SO6p4de"},"source":["# 埋め込み表現の可視化"]},{"cell_type":"markdown","metadata":{"id":"0VbIPNUGq9VG"},"source":["### embeddingの取得"]},{"cell_type":"code","metadata":{"id":"Cir_AqBtMYw7"},"source":["embeddings = []\n","filenames = []\n","\n","# disable gradients for faster calculations\n","model.eval()\n","with torch.no_grad():\n","    for i, (x, _, fnames) in enumerate(dataloader_test):\n","        # move the images to the gpu\n","        x = x.to(device)\n","        # embed the images with the pre-trained backbone\n","        y = model.backbone(x)\n","        y = y.squeeze()\n","        # store the embeddings and filenames in lists\n","        embeddings.append(y)\n","        filenames = filenames + list(fnames)\n","\n","# concatenate the embeddings and convert to numpy\n","embeddings = torch.cat(embeddings, dim=0)\n","embeddings = embeddings.cpu().numpy()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Rez0xwevrBSk"},"source":["### plot用のライブラリ読み込み"]},{"cell_type":"code","metadata":{"id":"DDqIu44ZPbbt"},"source":["# for plotting\n","import os\n","from PIL import Image\n","\n","import matplotlib.pyplot as plt\n","import matplotlib.offsetbox as osb\n","from matplotlib import rcParams as rcp\n","\n","# for resizing images to thumbnails\n","import torchvision.transforms.functional as functional\n","\n","# for clustering and 2d representations\n","from sklearn import random_projection"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"EITZvXgZrHAb"},"source":["### embeddingの次元削減とNormalizing"]},{"cell_type":"code","metadata":{"id":"xb4ZHGAsPeyn"},"source":["# for the scatter plot we want to transform the images to a two-dimensional\n","# vector space using a random Gaussian projection\n","projection = random_projection.GaussianRandomProjection(n_components=2)\n","embeddings_2d = projection.fit_transform(embeddings)\n","\n","# normalize the embeddings to fit in the [0, 1] square\n","M = np.max(embeddings_2d, axis=0)\n","m = np.min(embeddings_2d, axis=0)\n","embeddings_2d = (embeddings_2d - m) / (M - m)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"yfixduWsrMZ_"},"source":["### 散布図形式のサムネイル可視化"]},{"cell_type":"code","metadata":{"id":"uV8u8C0aPiKk"},"source":["def get_scatter_plot_with_thumbnails():\n","    \"\"\"Creates a scatter plot with image overlays.\n","    \"\"\"\n","    # initialize empty figure and add subplot\n","    fig = plt.figure(figsize=(12,12))\n","    fig.suptitle('SimSiam Scatter Plot')\n","    ax = fig.add_subplot(1, 1, 1)\n","    # shuffle images and find out which images to show\n","    shown_images_idx = []\n","    shown_images = np.array([[1., 1.]])\n","    iterator = [i for i in range(embeddings_2d.shape[0])]\n","    np.random.shuffle(iterator)\n","    for i in iterator:\n","        # only show image if it is sufficiently far away from the others\n","        dist = np.sum((embeddings_2d[i] - shown_images) ** 2, 1)\n","        if np.min(dist) < 1.5e-3:\n","            continue\n","        shown_images = np.r_[shown_images, [embeddings_2d[i]]]\n","        shown_images_idx.append(i)\n","\n","    # plot image overlays\n","    for idx in shown_images_idx:\n","        thumbnail_size = int(rcp['figure.figsize'][0] * 5.)\n","        path = os.path.join(path_to_data, filenames[idx])\n","        img = Image.open(path)\n","        img = functional.resize(img, thumbnail_size)\n","        img = np.array(img)\n","        img_box = osb.AnnotationBbox(\n","            osb.OffsetImage(img, cmap=plt.cm.gray_r),\n","            embeddings_2d[idx],\n","            pad=0.2,\n","        )\n","        ax.add_artist(img_box)\n","\n","    # set aspect ratio\n","    ratio = 1. / ax.get_data_ratio()\n","    ax.set_aspect(ratio, adjustable='box')\n","\n","\n","# get a scatter plot with thumbnail overlays\n","get_scatter_plot_with_thumbnails()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Tt6tiXxxKXtQ"},"source":["比較的近い色調・表現の絵同士が集まっており、表現学習が行えていることが分かります"]},{"cell_type":"markdown","metadata":{"id":"1fGGiLrErSKn"},"source":["### 類似画像の可視化"]},{"cell_type":"code","metadata":{"id":"kNUz_r3_PkPO"},"source":["def get_image_as_np_array(filename: str):\n","    \"\"\"Loads the image with filename and returns it as a numpy array.\n","\n","    \"\"\"\n","    img = Image.open(filename)\n","    return np.asarray(img)[...,:3]\n","\n","\n","def get_image_as_np_array_with_frame(filename: str, w: int = 5):\n","    \"\"\"Returns an image as a numpy array with a black frame of width w.\n","\n","    \"\"\"\n","    img = get_image_as_np_array(filename)\n","    ny, nx, _ = img.shape\n","    # create an empty image with padding for the frame\n","    framed_img = np.zeros((w + ny + w, w + nx + w, 3))\n","    framed_img = framed_img.astype(np.uint8)\n","    # put the original image in the middle of the new one\n","    framed_img[w:-w, w:-w] = img\n","    return framed_img\n","\n","\n","def plot_nearest_neighbors_3x3(example_image: str, i: int):\n","    \"\"\"Plots the example image and its eight nearest neighbors.\n","\n","    \"\"\"\n","    n_subplots = 9\n","    # initialize empty figure\n","    fig = plt.figure()\n","    fig.suptitle(f\"Nearest Neighbor Plot {i + 1}\")\n","    #\n","    example_idx = filenames.index(example_image)\n","    # get distances to the cluster center\n","    distances = embeddings - embeddings[example_idx]\n","    distances = np.power(distances, 2).sum(-1).squeeze()\n","    # sort indices by distance to the center\n","    nearest_neighbors = np.argsort(distances)[:n_subplots]\n","    # show images\n","    for plot_offset, plot_idx in enumerate(nearest_neighbors):\n","        ax = fig.add_subplot(3, 3, plot_offset + 1)\n","        # get the corresponding filename\n","        fname = os.path.join(path_to_data, filenames[plot_idx])\n","        if plot_offset == 0:\n","            ax.set_title(f\"Example Image\")\n","            plt.imshow(get_image_as_np_array_with_frame(fname))\n","        else:\n","            plt.imshow(get_image_as_np_array(fname))\n","        # let's disable the axis\n","        plt.axis(\"off\")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"jA8k9xKaQlvn"},"source":["# show example images for each cluster\n","example_images = [\n","    '0a207830d8cca27de4be.jpg',\n","    '000bd5e82eb22f199f44.jpg',\n","    '4193ebdc9a860f646a40.jpg',\n","    '0cd8af895677b51c5897.jpg',\n","    '0a44488ae1db7d79d033.jpg',\n","]\n","\n","for i, example_image in enumerate(example_images):\n","    plot_nearest_neighbors_3x3(example_image, i)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ZzNas_51K4Vd"},"source":["人物画・花・生き物などがそれぞれ類似画像として提案されているようです"]},{"cell_type":"markdown","metadata":{"id":"c-T6BDDeLKRn"},"source":["## 学習済みモデルの活用"]},{"cell_type":"markdown","metadata":{"id":"zkNBdBunLSTj"},"source":["SimSiamによる学習済みのResNet18は`model.backbone`として呼び出すことができます  。  \n","これを用いて更に教師あり学習を行ったり、学習済みembeddingを推論に活用することが可能です。  \n","（教師あり学習を行う場合は最終層にLinear層を追加する必要があることに注意してください）"]},{"cell_type":"code","metadata":{"id":"sjrnr0moLNI7"},"source":["model.backbone"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"uWamDqwECyWM"},"source":[""],"execution_count":null,"outputs":[]}]}